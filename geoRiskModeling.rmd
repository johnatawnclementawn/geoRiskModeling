---
title: "Predictive Policing - Geospatial Risk Modeling"
author: "Johnathan Clementi"
date: "10/29/2021"
output: 
  html_document:
    code_folding: hide
    toc: TRUE
    toc_float: TRUE
---

We are going to run through the code base with just a couple variables in a model - in a slightly simplified workflow

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE)

library(tidyverse)
library(sf)
library(RSocrata)
library(viridis)
library(spatstat)
library(raster)
library(spdep)
library(FNN)
library(grid)
library(gridExtra)
library(knitr)
library(kableExtra)
library(tidycensus)

options(scipen =  "sf")
options(scipen = 999)

# functions
root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"
source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

```

## Read in Data from Washington DC

```{r DCBoundaries, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
crs <- 'ESRI:102285' #StatePlane Maryland

# Overall DC Boundary
DCBoundary <- 
  st_read("https://opendata.arcgis.com/datasets/7241f6d500b44288ad983f0942b39663_10.geojson") %>%
  st_transform(crs) 

# DC Neighborhoods from github open data 
## Used in LOOCV 
neighborhoods <- 
  st_read("https://raw.githubusercontent.com/blackmad/neighborhoods/master/washington.geojson") %>%
  st_transform(crs) 

# Police Districts (7 districts) (larger spatial unit)
policeDistricts <- 
  st_read("https://opendata.arcgis.com/datasets/d2a63e5246ff41bdaca8ea9be95c8a4b_9.geojson") %>%
  st_transform(crs) %>%
  dplyr::select(District = DISTRICT) %>%
  mutate(District = as.character(District))
  
# Each police district has about 3 Sectors (smaller spatial unit) 
policeSectors <- 
  st_read("https://opendata.arcgis.com/datasets/6ac17c2ff8cc4e20b3768dd1b98adf7a_23.geojson") %>%
  st_transform(crs) %>%
  dplyr::select(District = NAME)

# Format police sectors in usable way
policeSectors$District <- paste0("0", str_replace(policeSectors$District, "D", "0"))


bothPoliceUnits <- rbind(mutate(policeDistricts, Legend = "Police Districts"), 
                         mutate(policeSectors, Legend = "Police Sectors"))

```

```{r graffiti, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
# DC 311 Complaints
DC311 <- 
  st_read("https://opendata.arcgis.com/datasets/19905e2b0e1140ec9ce8437776feb595_8.geojson") %>%
  st_transform(crs)  %>%
  dplyr::select(-c("INSPECTIONFLAG", "INSPECTIONDATE", "INSPECTORNAME")) 

DC311Vars <- data.frame(table(DC311$SERVICECODEDESCRIPTION))

# Pull graffiti from overall 311 data
graffiti <- DC311 %>%
  filter(grepl("Graffiti", SERVICECODEDESCRIPTION)) %>%
  mutate(Count = 1) %>%
  dplyr::select(Description = SERVICECODEDESCRIPTION, Count)

```

## visualizing point data

Plotting point data and density

> How do we analyze point data?
>
> Are there other geometries useful to represent point locations?
  - aggregate by 

```{r pointDens, echo=TRUE, fig.height=6, fig.width=7, message=FALSE, warning=FALSE}

grid.arrange(ncol=2,
ggplot() + 
  geom_sf(data = DCBoundary) +
  geom_sf(data = graffiti, colour="red", size=0.1, show.legend = "point") +
  labs(title= "Graffiti Complaints,\nWashington DC - 2017") +
  mapTheme(title_size = 14),

ggplot() + 
  geom_sf(data = DCBoundary, fill = "grey40") +
  stat_density2d(data = data.frame(st_coordinates(graffiti)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_viridis() +
  scale_alpha(range = c(0.00, 0.35), guide = FALSE) +
  labs(title = "Density of Graffiti Complaints\nWashington DC - 2017") +
  mapTheme(title_size = 14) + theme(legend.position = "none"))

```

## Creating a fishnet grid

```{r fishnet, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
## using {sf} to create the grid
## Note the `.[DCBoundary] %>% ` line. This is needed to clip the grid to our data
fishnet <- 
  st_make_grid(DCBoundary,
               cellsize = 500, 
               square = TRUE) %>%
  .[DCBoundary] %>%            # <- MDH Added
  st_sf() %>%
  mutate(uniqueID = rownames(.))
```

### Aggregate points to the fishnet

```{r graffitiAggregate, echo=TRUE echo=TRUE, message=FALSE, warning=FALSE}
## add a value of 1 to each crime, sum them with aggregate

crime_net <- 
  dplyr::select(graffiti) %>% 
  mutate(countGraffiti = 1) %>% 
  aggregate(., fishnet, sum) %>%
  mutate(countGraffiti = replace_na(countGraffiti, 0),
         uniqueID = rownames(.),
         cvID = sample(round(nrow(fishnet) / 24), 
                       size=nrow(fishnet), replace = TRUE))

ggplot() +
  geom_sf(data = crime_net, aes(fill = countGraffiti), color = NA) +
  scale_fill_viridis() +
  labs(title = "Count of Graffiti complaints for the fishnet") +
  mapTheme()

```

## Modeling Spatial Features

> What features would be helpful in predicting the location of burglaries?
>
> What might these features be problematic?
>
> hint: for all the reasons we learned in class


```{r pullGraffitiPredictors, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}

# Abandoned cars (includes both public & private land)
abandonedCars <- DC311 %>%
  filter(grepl("Abandoned Vehicle", SERVICECODEDESCRIPTION)) %>%
  mutate(Legend = "Abandoned_Car", Count = 1) %>%
  dplyr::select(Legend, Count)

# Pull Illegal Dumping from overall 311 data
Illegal_Dumping <- DC311 %>%
  filter(grepl("Illegal Dumping", SERVICECODEDESCRIPTION)) %>%
  mutate(Legend = "Illegal_Dumping", Count = 1) %>%
  dplyr::select(Legend, Count)

# Maybe drunk people are more likely to tag?
liquourLicenses <- st_read("https://opendata.arcgis.com/datasets/cabe9dcef0b344518c7fae1a3def7de1_5.geojson") %>%
  st_transform(crs) %>%
  mutate(Legend = "liquourLicenses", Count = 1) %>%
  dplyr::select(Legend, Count)

# Metro stations 
metroStops <- st_read("https://opendata.arcgis.com/datasets/54018b7f06b943f2af278bbe415df1de_52.geojson") %>%
	st_transform(crs) %>%
  mutate(Legend = "metroStops", Count = 1) %>%
  dplyr::select(Legend, Count)

# 311 complaints regarding streetlight outages
streetlightsOut <- DC311%>%
  filter(grepl("Streetlight", SERVICECODEDESCRIPTION)) %>%
  mutate(Legend = "streetlightsOut", Count = 1) %>%
  dplyr::select(Legend, Count)

# Major crimes 
thefts <- st_read("https://opendata.arcgis.com/datasets/6af5cb8dc38e4bcbac8168b27ee104aa_38.geojson") %>%
  st_transform(crs) %>%
	filter(grepl('THEFT',OFFENSE)) %>%
	mutate(Legend = "thefts", Count = 1) %>%
	dplyr::select(Legend, Count) 


# create single dataset for all predictors
allPredictors <- rbind(abandonedCars, Illegal_Dumping, liquourLicenses, metroStops, streetlightsOut, thefts)
```
#### Raw predictor variables
```{r}
vars <- unique(allPredictors$Legend)
mapList <- list()

for(i in vars){
  mapList[[i]] <- 
    ggplot() +
      geom_sf(data = DCBoundary) +
      geom_sf(data = filter(allPredictors, Legend == i), size = 0.2) +
      labs(title=i) +
      mapTheme()}

do.call(grid.arrange,c(mapList, ncol = 3, top = "Risk factors"))
```



#### How we aggregate a feature to our fishnet

This is an important chunk of code with some unfamiliar lines. We'll step through it.

```{r echo=TRUE, message=FALSE, warning=FALSE, results='hide'}

vars_net <- allPredictors %>%
  st_join(., fishnet, join=st_within) %>%
  st_drop_geometry() %>%
  group_by(uniqueID, Legend) %>%
  summarize(count = n()) %>%
  full_join(fishnet, by = "uniqueID") %>%
  spread(Legend, count, fill=0) %>%
  st_sf() %>%
  dplyr::select(-`<NA>`) %>%
  na.omit() %>%
  ungroup()

# vars_net <- allPredictors %>%
#   spatially join allPredictors points to the fishnet polygon they are within %>%
#   drop the geometry attribute %>%
#   group_by each cells ID and the name of the feature %>%
#   summarize count the number of each point per grid cell %>%
#   join that summary back to spatial fishnet by cell ID %>%
#   "spread" from long to wide format and make column of our point count %>%
#   tell R that this should be an sf object %>%
#   remove a fussy column that appears b/c of NA %>%
#   get rid of rows with an NA in any column %>%
#   remove grouping so you are not tripped up later
```

## Nearest Neighbor Feature

> Review: what is NN and what does `k` represent in this function?

```{r}
# convinience to reduce length of function names.
st_c    <- st_coordinates
st_coid <- st_centroid

# Get coordinates of fishnet centroids
netCentroidCoords <- st_c(st_coid(vars_net))

# Get coordinates for k-nearest neighbor functions below
accoords <- st_c(abandonedCars)
idcoords <- st_c(Illegal_Dumping)
lqcoords <- st_c(liquourLicenses)
mscoords <- st_c(metroStops)
slcoords <- st_c(streetlightsOut)
tcoords <- st_c(thefts)

# Define number of nearest neighbors to look for
k = 3
                    

## create NN from abandoned cars
vars_net <- vars_net %>%
    mutate(Abandoned_Cars.nn = nn_function(netCentroidCoords, accoords, k), 
           Illegal_Dumping.nn = nn_function(netCentroidCoords, idcoords, k),
           liquourLicenses.nn = nn_function(netCentroidCoords, lqcoords, k),
           metroStops.nn = nn_function(netCentroidCoords, mscoords, k),
           streetlightsOut.nn = nn_function(netCentroidCoords, slcoords, k),
           thefts.nn = nn_function(netCentroidCoords, tcoords, k)
          )                                             

```


```{r}
## Visualize the NN feature
vars_net.long.nn <- 
  dplyr::select(vars_net, ends_with(".nn")) %>%
    gather(Variable, value, -geometry)

vars <- unique(vars_net.long.nn$Variable)
mapList <- list()

for(i in vars){
  mapList[[i]] <- 
    ggplot() +
      geom_sf(data = filter(vars_net.long.nn, Variable == i), aes(fill=value), colour=NA) +
      scale_fill_viridis(name="") +
      labs(title=i) +
      mapTheme()}

do.call(grid.arrange,c(mapList, ncol = 3, top = "Average Distance to nearest neighbor(s) risk factors by fishnet"))
```

## Join NN feature to our fishnet

Since the counts were aggregated to each cell by `uniqueID` we can use that to join the counts to the fishnet.

```{r}
## important to drop the geometry from joining features
final_net <-
  left_join(crime_net, st_drop_geometry(vars_net), by="uniqueID") 

```

### Join in areal data

Using spatial joins to join *centroids* of fishnets to polygon for neighborhoods and districts.

> What issues arise when we try to join polygons to polygons in space? End up with weird slices of polygons

```{r}

final_net <-
  st_centroid(final_net) %>%
    st_join(dplyr::select(neighborhoods, name), by = "uniqueID") %>%
    st_join(dplyr::select(policeDistricts, District), by = "uniqueID") %>%
      st_drop_geometry() %>%
      left_join(dplyr::select(final_net, geometry, uniqueID)) %>%
      st_sf() %>%
  na.omit()

```



## Local Moran's I for fishnet grid cells

using {spdep} package to to build neighborhood weights and list to calculate local Moran's I.

Note that the code here is *different* than in the book - it has been updated to keep up with changes in packages.

> What is the difference between local and global Moran's I?

A little in depth version of the chunk below can be found:

Mendez C. (2020). Spatial autocorrelation analysis in R. R Studio/RPubs. Available at <https://rpubs.com/quarcs-lab/spatial-autocorrelation>

```{r}
## generates warnings from PROJ issues
## {spdep} to make polygon to neighborhoods... 
final_net.nb <- poly2nb(as_Spatial(final_net), queen=TRUE)
## ... and neighborhoods to list of weigths
final_net.weights <- nb2listw(final_net.nb, style="W", zero.policy=TRUE)

# print(final_net.weights, zero.policy=TRUE)
```

```{r}
## see ?localmoran
local_morans <- localmoran(final_net$countGraffiti, final_net.weights, zero.policy=TRUE) %>% 
  as.data.frame()

# join local Moran's I results to fishnet
final_net.localMorans <- 
  cbind(local_morans, as.data.frame(final_net)) %>% 
  st_sf() %>%
  dplyr::select(Graffiti_Count = countGraffiti, 
                Local_Morans_I = Ii, 
                P_Value = `Pr(z != E(Ii))`) %>%
  mutate(Significant_Hotspots = ifelse(P_Value <= 0.001, 1, 0)) %>%
  gather(Variable, Value, -geometry)
  
```

### Plotting local Moran's I results

This is a complex code chunk - it's a loop which builds ggplots of local Moran's for each of your `vars`

> What does a significant hot spot tell us about the distribution of burglaries?

```{r}
## This is just for plotting
vars <- unique(final_net.localMorans$Variable)
varList <- list()

for(i in vars){
  varList[[i]] <- 
    ggplot() +
      geom_sf(data = filter(final_net.localMorans, Variable == i), 
              aes(fill = Value), colour=NA) +
      scale_fill_viridis(name="") +
      labs(title=i) +
      mapTheme(title_size = 14) + theme(legend.position="bottom")}

do.call(grid.arrange,c(varList, ncol = length(vars), top = "Local Morans I statistics, Graffiti"))
```

## Distance to graffiti hot spots

Using NN distance to a hot spot location

```{r}
# generates warning from NN
final_net <- final_net %>% 
  mutate(graffiti.isSig = 
           ifelse(local_morans[,5] <= 0.001, 1, 0)) %>%
  mutate(graffiti.isSig.dist = 
           nn_function(st_c(st_coid(final_net)),
                       st_c(st_coid(filter(final_net, 
                                           graffiti.isSig == 1))), 
                       k = 1))

ggplot() +
      geom_sf(data = final_net, aes(fill=graffiti.isSig.dist), colour=NA) +
      scale_fill_viridis(name="NN Distance") +
      labs(title="Distance to Graffiti Hotspots") +
      mapTheme()
```

## Correlation Small Multiples
```{r correlationSM, fig.height=10, fig.width= 6}
correlation.long <-
  st_drop_geometry(final_net) %>%
    dplyr::select(-uniqueID, -cvID, -name, -District) %>%
    gather(Variable, Value, -countGraffiti)

correlation.cor <-
  correlation.long %>%
    group_by(Variable) %>%
    summarize(correlation = cor(Value, countGraffiti, use = "complete.obs"))
    
ggplot(correlation.long, aes(Value, countGraffiti)) +
  geom_point(size = 0.1) +
  geom_text(data = correlation.cor, aes(label = paste("r =", round(correlation, 2))),
            x=-Inf, y=Inf, vjust = 1.5, hjust = -.1) +
  geom_smooth(method = "lm", se = FALSE, colour = "black") +
  facet_wrap(~Variable, ncol = 2, scales = "free") +
  labs(title = "Graffiti count as a function of risk factors") +
  plotTheme()
```


## Modeling and CV

Leave One Group Out CV on spatial features

```{r}
# Edited function from textbook to get rid of hard-coded variable
crossValidate <- function(dataset, id, dependentVariable, indVariables) {
  
  allPredictions <- data.frame()
  cvID_list <- unique(dataset[[id]])
  
  for (i in cvID_list) {
    
    thisFold <- i
    cat("This hold out fold is", thisFold, "\n")
    
    fold.train <- filter(dataset, dataset[[id]] != thisFold) %>% as.data.frame() %>% 
      dplyr::select(id, geometry, indVariables, dependentVariable)
    fold.test  <- filter(dataset, dataset[[id]] == thisFold) %>% as.data.frame() %>% 
      dplyr::select(id, geometry, indVariables, dependentVariable)
    
    regression <-
      glm(paste0(dependentVariable,"~."), family = "poisson", 
          data = fold.train %>% 
            dplyr::select(-geometry, -id))
    
    thisPrediction <- 
      mutate(fold.test, Prediction = predict(regression, fold.test, type = "response"))
    
    allPredictions <-
      rbind(allPredictions, thisPrediction)
    
  }
  return(st_sf(allPredictions))
}

```


```{r sults='hide'}

## define the variables we want
reg.vars <- c("Abandoned_Cars.nn", "Illegal_Dumping.nn", 
              "liquourLicenses.nn", "metroStops.nn", "streetlightsOut.nn", "thefts.nn")

reg.ss.vars <- c("Abandoned_Cars.nn", "Illegal_Dumping.nn", 
                "liquourLicenses.nn", "metroStops.nn", "streetlightsOut.nn", "thefts.nn",
                "graffiti.isSig", "graffiti.isSig.dist")

## RUN REGRESSIONS
reg.cv <- crossValidate(
  dataset = final_net,
  id = "cvID",
  dependentVariable = "countGraffiti",
  indVariables = reg.vars) %>%
    dplyr::select(cvID = cvID, countGraffiti, Prediction, geometry)

reg.ss.cv <- crossValidate(
  dataset = final_net,
  id = "cvID",
  dependentVariable = "countGraffiti",
  indVariables = reg.ss.vars) %>%
    dplyr::select(cvID = cvID, countGraffiti, Prediction, geometry)
  
reg.spatialCV <- crossValidate(
  dataset = final_net,
  id = "name",
  dependentVariable = "countGraffiti",
  indVariables = reg.vars) %>%
    dplyr::select(cvID = name, countGraffiti, Prediction, geometry)

reg.ss.spatialCV <- crossValidate(
  dataset = final_net,
  id = "name",                           
  dependentVariable = "countGraffiti",
  indVariables = reg.ss.vars) %>%
    dplyr::select(cvID = name, countGraffiti, Prediction, geometry)
```

#### Test Accuracy and Generalizability
```{r}
reg.summary <- 
  rbind(
    mutate(reg.cv,           Error = Prediction - countGraffiti,
                             Regression = "Random k-fold CV: Just Risk Factors"),
                             
    mutate(reg.ss.cv,        Error = Prediction - countGraffiti,
                             Regression = "Random k-fold CV: Spatial Process"),
    
    mutate(reg.spatialCV,    Error = Prediction - countGraffiti,
                             Regression = "Spatial LOGO-CV: Just Risk Factors"),
                             
    mutate(reg.ss.spatialCV, Error = Prediction - countGraffiti,
                             Regression = "Spatial LOGO-CV: Spatial Process")) %>%
    st_sf() 
```


```{r}
# calculate errors by NEIGHBORHOOD
error_by_reg_and_fold <- 
  reg.summary %>%
    group_by(Regression, cvID) %>% 
    summarize(Mean_Error = mean(Prediction - countGraffiti, na.rm = T),
              MAE = mean(abs(Mean_Error), na.rm = T),
              SD_MAE = mean(abs(Mean_Error), na.rm = T)) %>%
  ungroup()

## plot histogram of OOF (out of fold) errors
error_by_reg_and_fold %>%
  ggplot(aes(MAE)) + 
    geom_histogram(bins = 30, colour="black", fill = "#FDE725FF") +
    facet_wrap(~Regression) +  
    geom_vline(xintercept = 0) + scale_x_continuous(breaks = seq(0, 8, by = 1)) + 
    labs(title="Distribution of MAE", subtitle = "k-fold cross validation vs. LOGO-CV",
         x="Mean Absolute Error", y="Count") +
    plotTheme()
```
#### MAE & MAPE by regression
```{r}
st_drop_geometry(error_by_reg_and_fold) %>%
  group_by(Regression) %>% 
    summarize(Mean_MAE = round(mean(MAE), 2),
              SD_MAE = round(sd(MAE), 2)) %>%
  kable() %>%
    kable_styling("striped", full_width = F) %>%
    row_spec(2, color = "black", background = "#FDE725FF") %>%
    row_spec(4, color = "black", background = "#FDE725FF")
```
## Racial Context
```{r}
census_var2017 <- c("B25026_001E", "B02001_002E", "B19013_001E", "B25058_001E", "B06012_002E")
tractsDC.17 <- get_acs(geography = "tract", 
                           variables = census_var2017, 
                           year=2017, 
                           state=11999,
                           geometry=TRUE, 
                           output="wide") %>%
	dplyr::select(-geometry, -B25026_001M, -B02001_002M, -B19013_001M, -B25058_001M, -B06012_002M) %>% # Remove margin of error columns
	st_transform(crs) %>%
  rename(TotalPop = B25026_001E, 
  			 Whites = B02001_002E,
         MedHHInc = B19013_001E, 
  			 MedRent = B25058_001E,
         TotalPoverty = B06012_002E) %>%
  #dplyr::select(-NAME, -starts_with("B")) %>%
  mutate(pctWhite = ifelse(TotalPop > 0, Whites / TotalPop,0),
         pctPoverty = ifelse(TotalPop > 0, TotalPoverty / TotalPop, 0),
         year = "2017") %>%
	mutate(pctWhite.scaled = pctWhite * 100,
				 pctPoverty.scaled = pctPoverty * 100,
				 raceContext = ifelse(pctWhite > .5, "Majority_White", "Majority_Non_White")) %>%
  dplyr::select(-Whites, -TotalPoverty, -NAME)
```
### Mean Errors by Majority/Minority White Tracts
```{r}
reg.summary %>%
  filter(str_detect(Regression, "LOGO")) %>%
  st_centroid() %>%
  st_join(tractsDC.17) %>%
  na.omit() %>%
  st_drop_geometry() %>%
  group_by(Regression, raceContext) %>%
  summarize(mean.Error = mean(Error, na.rm = T)) %>%
  spread(raceContext, mean.Error) %>%
  kable(caption = "Mean Error by neighborhood racial demographics") %>%
  kable_styling("striped", full_width = F)


```



## Density vs predictions

The `spatstat` function gets us kernel density estimates with varying search radii.

Note that the code here is *different* than in the book - it has been updated to keep up with changes in packages.

```{r}
# demo of kernel width
graf_ppp <- as.ppp(st_coordinates(graffiti), W = st_bbox(final_net))
graf_KD.1000 <- spatstat.core::density.ppp(graf_ppp, 1000)
graf_KD.1500 <- spatstat.core::density.ppp(graf_ppp, 1500)
graf_KD.2000 <- spatstat.core::density.ppp(graf_ppp, 2000)
graf_KD.df <- rbind(
  mutate(data.frame(rasterToPoints(mask(raster(graf_KD.1000), as(neighborhoods, 'Spatial')))), Legend = "1000 Ft."),
  mutate(data.frame(rasterToPoints(mask(raster(graf_KD.1500), as(neighborhoods, 'Spatial')))), Legend = "1500 Ft."),
  mutate(data.frame(rasterToPoints(mask(raster(graf_KD.2000), as(neighborhoods, 'Spatial')))), Legend = "2000 Ft.")) 

graf_KD.df$Legend <- factor(graf_KD.df$Legend, levels = c("1000 Ft.", "1500 Ft.", "2000 Ft."))

ggplot(data=graf_KD.df, aes(x=x, y=y)) +
  geom_raster(aes(fill=layer)) + 
  facet_wrap(~Legend) +
  coord_sf(crs=st_crs(final_net)) + 
  scale_fill_viridis(name="Density") +
  labs(title = "Kernel density with 3 different search radii") +
  mapTheme(title_size = 14)
```

```{r}

as.data.frame(graf_KD.1000) %>%
  st_as_sf(coords = c("x", "y"), crs = st_crs(final_net)) %>%
  aggregate(., final_net, mean) %>%
   ggplot() +
     geom_sf(aes(fill=value)) +
     geom_sf(data = sample_n(graffiti, 1500), size = .5) +
     scale_fill_viridis(name = "Density") +
     labs(title = "Kernel density of 2017 graffiti") +
     mapTheme(title_size = 14)
```











## Get 2018 crime data

Let's see how our model performed relative to KD on the following year's data.

```{r}
DC311_18 <- st_read("https://opendata.arcgis.com/datasets/2a46f1f1aad04940b83e75e744eb3b09_9.geojson") %>%
  st_transform(crs) %>%
  dplyr::select(-c("INSPECTIONFLAG", "INSPECTIONDATE", "INSPECTORNAME"))

# Pull graffiti from overall 311 data
graffiti18 <- DC311_18 %>%
  filter(grepl("Graffiti", SERVICECODEDESCRIPTION)) %>%
  dplyr::select(Description = SERVICECODEDESCRIPTION)

crime_net_18 <- 
  dplyr::select(graffiti18) %>%
  mutate(countGraffiti = 1) %>%
  aggregate(., fishnet, sum) %>%
  mutate(countGraffiti = replace_na(countGraffiti, 0),
         uniqueID = rownames(.),
         cvID = sample(round(nrow(fishnet) / 24), 
                       size=nrow(fishnet), replace = TRUE))

```

```{r}
graf_KDE_sf <- as.data.frame(graf_KD.1000) %>%
  st_as_sf(coords = c("x", "y"), crs = st_crs(final_net)) %>%
  aggregate(., final_net, mean) %>%
  mutate(label = "Kernel Density",
         Risk_Category = ntile(value, 100),
         Risk_Category = case_when(
           Risk_Category >= 90 ~ "90% to 100%",
           Risk_Category >= 70 & Risk_Category <= 89 ~ "70% to 89%",
           Risk_Category >= 50 & Risk_Category <= 69 ~ "50% to 69%",
           Risk_Category >= 30 & Risk_Category <= 49 ~ "30% to 49%",
           Risk_Category >= 1 & Risk_Category  <= 29 ~ "1% to 29%")) %>%
  cbind(
    aggregate(
      dplyr::select(graffiti18) %>% mutate(grafCount = 1), ., sum) %>%
    mutate(grafCount = replace_na(grafCount, 0))) %>%
  dplyr::select(label, Risk_Category, grafCount)
```

Note that this is different from the book, where we pull a model out of a list of models we've created. For your homework, you'll be creating multiple models.

```{r}
graf_risk_sf <-
  reg.ss.spatialCV %>%
  mutate(label = "Risk Predictions",
         Risk_Category = ntile(Prediction, 100),
         Risk_Category = case_when(
         Risk_Category >= 90 ~ "90% to 100%",
         Risk_Category >= 70 & Risk_Category <= 89 ~ "70% to 89%",
         Risk_Category >= 50 & Risk_Category <= 69 ~ "50% to 69%",
         Risk_Category >= 30 & Risk_Category <= 49 ~ "30% to 49%",
         Risk_Category >= 1 & Risk_Category <= 29 ~ "1% to 29%")) %>%
  cbind(
    aggregate(
      dplyr::select(graffiti18) %>% mutate(grafCount = 1), ., sum) %>%
      mutate(grafCount = replace_na(grafCount, 0))) %>%
  dplyr::select(label,Risk_Category, grafCount)
```

We don't do quite as well because we don't have very many features, but still pretty good.

```{r}
rbind(graf_KDE_sf, graf_risk_sf) %>%
  na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category, -geometry) %>%
  ggplot() +
    geom_sf(aes(fill = Risk_Category), colour = NA) +
    geom_sf(data = sample_n(graffiti18, 3000), size = .5, colour = "black") +
    facet_wrap(~label, ) +
    scale_fill_viridis(discrete = TRUE) +
    labs(title="Comparison of Kernel Density and Risk Predictions",
         subtitle="2017 graffiti risk predictions; 2018 graffiti") +
    mapTheme(title_size = 14)
```

```{r}
rbind(graf_KDE_sf, graf_risk_sf) %>%
  st_set_geometry(NULL) %>% na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category) %>%
  group_by(label, Risk_Category) %>%
  summarize(countGraffiti = sum(Value)) %>%
  ungroup() %>%
  group_by(label) %>%
  mutate(Rate_of_test_set_crimes = countGraffiti / sum(countGraffiti)) %>%
    ggplot(aes(Risk_Category,Rate_of_test_set_crimes)) +
      geom_bar(aes(fill=label), position="dodge", stat="identity") +
      scale_fill_viridis(discrete = TRUE) +
      labs(title = "Risk prediction vs. Kernel density, 2018 graffiti") +
      theme(axis.text.x = element_text(angle = 45, vjust = 0.5))
```
