---
title: "Predictive Policing - Geospatial Risk Modeling"
author: "Johnathan Clementi"
date: "10/29/2021"
output: 
  html_document:
    theme: journal
    highlight: haddock
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
    code_folding: hide
---
# Introduction   

This workflow is primarily designed for city or county police, sheriff, and/or 311 departments. Traditionally, these departments have either used rudimentary methods of predicting where crimes or disturbances will occur or not used any predictions at all. The methods of prediction usually entail hot-spot mapping through kernel density analysis. In this workflow, we will try to build a model using regression techniques to predict where crimes will occur, and thus allow for more effective and efficient deployment of city or county resources. The R-markdown file is built for someone who has some experience with R and so they can ‘plug-and-play’ the data for their respective use case. The code is available by clicking on the **code** button.   

We will build the model using data from 2017, test its accuracy using cross validation techniques, and then determine if it generalizable to future scenarios by testing it using data form 2018. We chose to model 311 Complaints about graffiti for its possible issues with selection bias. Said differently, what is classified as graffiti and what is classified as art is subjective. Therefore, we would expect to see some spatial clustering of graffiti complaints based on outside factors. This is what we will try to account for in our model.     

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
library(FNN)
library(ggforce)
library(grid)
library(gridExtra)
library(kableExtra)
library(knitr)
library(raster)
library(RSocrata)
library(sf)
library(spatstat)
library(spdep)
library(tidycensus)
library(tidyverse)
library(viridis)

options(scipen =  "sf")
options(scipen = 999)

# functions
root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"
source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

```

# Methods   

### Data Retrieval and Cleaning   
These data come from OpenDataDC and a github repository maintained by David Blackman (DC Neighborhoods).   

```{r DCBoundaries, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
crs <- 'ESRI:102285' #StatePlane Maryland

# Overall DC Boundary
DCBoundary <- 
  st_read("https://opendata.arcgis.com/datasets/7241f6d500b44288ad983f0942b39663_10.geojson") %>%
  st_transform(crs) 

# DC Neighborhoods from github open data 
## Used in LOOCV 
neighborhoods <- 
  st_read("https://raw.githubusercontent.com/blackmad/neighborhoods/master/washington.geojson") %>%
  st_transform(crs) 

# Police Districts (7 districts) (larger spatial unit)
policeDistricts <- 
  st_read("https://opendata.arcgis.com/datasets/d2a63e5246ff41bdaca8ea9be95c8a4b_9.geojson") %>%
  st_transform(crs) %>%
  dplyr::select(District = DISTRICT) %>%
  mutate(District = as.character(District))
  
# Each police district has about 3 Sectors (smaller spatial unit) 
policeSectors <- 
  st_read("https://opendata.arcgis.com/datasets/6ac17c2ff8cc4e20b3768dd1b98adf7a_23.geojson") %>%
  st_transform(crs) %>%
  dplyr::select(District = NAME)

# Format police sectors in usable way
policeSectors$District <- paste0("0", str_replace(policeSectors$District, "D", "0"))


bothPoliceUnits <- rbind(mutate(policeDistricts, Legend = "Police Districts"), 
                         mutate(policeSectors, Legend = "Police Sectors"))


# DC 311 Complaints
DC311 <- 
  st_read("https://opendata.arcgis.com/datasets/19905e2b0e1140ec9ce8437776feb595_8.geojson") %>%
  st_transform(crs)  %>%
  dplyr::select(-c("INSPECTIONFLAG", "INSPECTIONDATE", "INSPECTORNAME")) 

DC311Vars <- data.frame(table(DC311$SERVICECODEDESCRIPTION))

# Pull graffiti from overall 311 data
graffiti <- DC311 %>%
  filter(grepl("Graffiti", SERVICECODEDESCRIPTION)) %>%
  mutate(Count = 1) %>%
  dplyr::select(Description = SERVICECODEDESCRIPTION, Count)

```

### Visualizing Distribution of Graffiti   
In the figure below, we have mapped both the raw locations of all 311 Graffiti removal complaints and the density of complaints across the Washington, DC for 2017.   

```{r pointDens, echo=TRUE, fig.height=6, fig.width=7, message=FALSE, warning=FALSE}

grid.arrange(ncol=2,
ggplot() + 
  geom_sf(data = DCBoundary) +
  geom_sf(data = graffiti, colour="red", size=0.1, show.legend = "point") +
  labs(title= "Graffiti Complaints,\nWashington DC - 2017") +
  mapTheme(title_size = 14),

ggplot() + 
  geom_sf(data = DCBoundary, fill = "grey40") +
  stat_density2d(data = data.frame(st_coordinates(graffiti)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_viridis() +
  scale_alpha(range = c(0.00, 0.35), guide = FALSE) +
  labs(title = "Density of Graffiti Complaints\nWashington DC - 2017") +
  mapTheme(title_size = 14) + theme(legend.position = "none"))

```

### Areas of Analysis   
For hotspot analysis, it is useful to first create a grid cell lattice known as a `fishnet`. The fishnet is useful in smoothly visualizing how phenomena varies across space. In the code and figure below, we have created the fishnet for Washington, DC and calculated the number of graffiti complaints that occurred within each fishnet cell in 2017.
```{r fishnet, echo=TRUE, message=FALSE, warning=FALSE}
## using {sf} to create the grid
## Note the `.[DCBoundary] %>% ` line. This is needed to clip the grid to our data

fishnet <- 
  st_make_grid(DCBoundary,
               cellsize = 500, 
               square = TRUE) %>%
  .[DCBoundary] %>%
  st_sf() %>%
  mutate(uniqueID = rownames(.))


crime_net <- 
  dplyr::select(graffiti) %>% 
  mutate(countGraffiti = 1) %>% 
  aggregate(., fishnet, sum) %>%
  mutate(countGraffiti = replace_na(countGraffiti, 0),
         uniqueID = rownames(.),
         cvID = sample(round(nrow(fishnet) / 24), 
                       size=nrow(fishnet), replace = TRUE))

ggplot() +
  geom_sf(data = crime_net, aes(fill = countGraffiti), color = NA) +
  scale_fill_viridis() +
  labs(title = "Count of Graffiti complaints for the fishnet") +
  mapTheme()

```   

The histogram below illustrates the distribution of graffiti as aggregated by the fishnet cells. Most of the fishnet cells have less zero to fifteen counts of graffiti in them. However, there are some cells in which there are over 250 occurrences of graffiti complaints.

```{r fishnetDist, echo=TRUE, message=FALSE, warning=FALSE}
# Distribution of Graffiti Complaints across fishnet
ggplot(data = crime_net, aes(countGraffiti))+
  geom_histogram(bins = 30, color = 'black', fill = "yellow") +
  scale_x_continuous(breaks = seq(0, 275, by = 15)) +
  labs(title = 'Distribution of graffiti complaints') +
  plotTheme()
```


## Modeling Spatial Features   

### Predictor variables
The data retrieved below will be used as predictor variables for building the model to predict the location of graffiti complaints.   

```{r pullGraffitiPredictors, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}

# Abandoned cars (includes both public & private land)
abandonedCars <- DC311 %>%
  filter(grepl("Abandoned Vehicle", SERVICECODEDESCRIPTION)) %>%
  mutate(Legend = "Abandoned_Car", Count = 1) %>%
  dplyr::select(Legend, Count)

# Alley Cleaning 
alleyCleaningReq <- DC311 %>%
  filter(grepl("Alley Cleaning", SERVICECODEDESCRIPTION)) %>%
  mutate(Legend = "AlleyCleaningReq", Count = 1) %>%
  dplyr::select(Legend, Count)

# Pull Illegal Dumping from overall 311 data
Illegal_Dumping <- DC311 %>%
  filter(grepl("Illegal Dumping", SERVICECODEDESCRIPTION)) %>%
  mutate(Legend = "Illegal_Dumping", Count = 1) %>%
  dplyr::select(Legend, Count)

# Maybe drunk people are more likely to tag?
liquourLicenses <- st_read("https://opendata.arcgis.com/datasets/cabe9dcef0b344518c7fae1a3def7de1_5.geojson") %>%
  st_transform(crs) %>%
  mutate(Legend = "liquourLicenses", Count = 1) %>%
  dplyr::select(Legend, Count)

# Metro stations 
metroStops <- st_read("https://opendata.arcgis.com/datasets/54018b7f06b943f2af278bbe415df1de_52.geojson") %>%
	st_transform(crs) %>%
  mutate(Legend = "metroStops", Count = 1) %>%
  dplyr::select(Legend, Count)

# 311 complaints regarding streetlight outages
streetlightsOut <- DC311%>%
  filter(grepl("Streetlight", SERVICECODEDESCRIPTION)) %>%
  mutate(Legend = "streetlightsOut", Count = 1) %>%
  dplyr::select(Legend, Count)

# Major crimes 
thefts <- st_read("https://opendata.arcgis.com/datasets/6af5cb8dc38e4bcbac8168b27ee104aa_38.geojson") %>%
  st_transform(crs) %>%
	filter(grepl('THEFT',OFFENSE)) %>%
	mutate(Legend = "thefts", Count = 1) %>%
	dplyr::select(Legend, Count) 


# create single dataset for all predictors
allPredictors <- rbind(alleyCleaningReq, Illegal_Dumping, liquourLicenses, metroStops, streetlightsOut, thefts)
```

#### Predictor variables visualized   

```{r predictorMaps, echo=TRUE, message=FALSE, warning=FALSE}
vars <- unique(allPredictors$Legend)
mapList <- list()

for(i in vars){
  mapList[[i]] <- 
    ggplot() +
      geom_sf(data = DCBoundary) +
      geom_sf(data = filter(allPredictors, Legend == i), size = 0.2) +
      labs(title=i) +
      mapTheme()}

do.call(grid.arrange,c(mapList, ncol = 3, top = "Risk factors"))
```

#### Merging predictors and the fishnet   
Much like we aggregated the dependent variable (graffiti complaints) by each fishnet cell, we also aggregate the predictor variables. The code below completes that task.   

```{r joinPredictors, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}

vars_net <- allPredictors %>%
  st_join(., fishnet, join=st_within) %>%
  st_drop_geometry() %>%
  group_by(uniqueID, Legend) %>%
  summarize(count = n()) %>%
  full_join(fishnet, by = "uniqueID") %>%
  spread(Legend, count, fill=0) %>%
  st_sf() %>%
  dplyr::select(-`<NA>`) %>%
  na.omit() %>%
  ungroup()

# vars_net <- allPredictors %>%
#   spatially join allPredictors points to the fishnet polygon they are within %>%
#   drop the geometry attribute %>%
#   group_by each cells ID and the name of the feature %>%
#   summarize count the number of each point per grid cell %>%
#   join that summary back to spatial fishnet by cell ID %>%
#   "spread" from long to wide format and make column of our point count %>%
#   tell R that this should be an sf object %>%
#   remove a fussy column that appears b/c of NA %>%
#   get rid of rows with an NA in any column %>%
#   remove grouping so you are not tripped up later
```

### Nearest Neighbor Features
We then calculate the average distance to the `k` number of nearest neighbors of each predictor variable to the centroid of the fishnet cell. These values are then visualized.   

```{r calcNN, echo=TRUE, message=FALSE, warning=FALSE}
# convinience to reduce length of function names.
st_c    <- st_coordinates
st_coid <- st_centroid

# Get coordinates of fishnet centroids
netCentroidCoords <- st_c(st_coid(vars_net))

# Get coordinates for k-nearest neighbor functions below
accoords <- st_c(abandonedCars)
alcoords <- st_c(alleyCleaningReq)
idcoords <- st_c(Illegal_Dumping)
lqcoords <- st_c(liquourLicenses)
mscoords <- st_c(metroStops)
slcoords <- st_c(streetlightsOut)
tcoords <- st_c(thefts)

# Define number of nearest neighbors to look for
k = 3
                    

## create NN from abandoned cars
vars_net <- vars_net %>%
    mutate(#Abandoned_Cars.nn = nn_function(netCentroidCoords, accoords, k),
           AlleyCleaning.nn = nn_function(netCentroidCoords, alcoords, k),
           Illegal_Dumping.nn = nn_function(netCentroidCoords, idcoords, k),
           liquourLicenses.nn = nn_function(netCentroidCoords, lqcoords, k),
           metroStops.nn = nn_function(netCentroidCoords, mscoords, k),
           streetlightsOut.nn = nn_function(netCentroidCoords, slcoords, k),
           thefts.nn = nn_function(netCentroidCoords, tcoords, k)
          )                                             

## Visualize the NN feature
vars_net.long.nn <- 
  dplyr::select(vars_net, ends_with(".nn")) %>%
    gather(Variable, value, -geometry)

vars <- unique(vars_net.long.nn$Variable)
mapList <- list()

for(i in vars){
  mapList[[i]] <- 
    ggplot() +
      geom_sf(data = filter(vars_net.long.nn, Variable == i), aes(fill=value), colour=NA) +
      scale_fill_viridis(name="") +
      labs(title=i) +
      mapTheme()}

do.call(grid.arrange,c(mapList, ncol = 3, top = "Average Distance to nearest neighbor(s) risk factors by fishnet"))
```

## Join NN feature to our fishnet

Since the counts were aggregated to each cell by `uniqueID` we can use that to join the counts to the fishnet.

```{r nnFishnet, echo=TRUE, message=FALSE, warning=FALSE}
## important to drop the geometry from joining features
final_net <-
  left_join(crime_net, st_drop_geometry(vars_net), by="uniqueID") 

```

### Join in areal data

Using spatial joins to join *centroids* of fishnets to polygon for neighborhoods and districts.

> What issues arise when we try to join polygons to polygons in space? End up with weird slices of polygons

```{r nbhdFishnet, echo=TRUE, message=FALSE, warning=FALSE}

final_net <-
  st_centroid(final_net) %>%
    st_join(dplyr::select(neighborhoods, name), by = "uniqueID") %>%
    st_join(dplyr::select(policeDistricts, District), by = "uniqueID") %>%
      st_drop_geometry() %>%
      left_join(dplyr::select(final_net, geometry, uniqueID)) %>%
      st_sf() %>%
  na.omit()

```



## Local Moran's I for fishnet grid cells

using {spdep} package to to build neighborhood weights and list to calculate local Moran's I.

```{r localMoransPrep, echo=TRUE, message=FALSE, warning=FALSE}
## generates warnings from PROJ issues
## {spdep} to make polygon to neighborhoods... 
final_net.nb <- poly2nb(as_Spatial(final_net), queen=TRUE)
## ... and neighborhoods to list of weigths
final_net.weights <- nb2listw(final_net.nb, style="W", zero.policy=TRUE)

# print(final_net.weights, zero.policy=TRUE)

# A little in depth version of the chunk below can be found:
# Mendez C. (2020). Spatial autocorrelation analysis in R. R Studio/RPubs. Available at <https://rpubs.com/quarcs-lab/spatial-autocorrelation>
```

```{r localMorans, echo=TRUE, message=FALSE, warning=FALSE}
## see ?localmoran
local_morans <- localmoran(final_net$countGraffiti, final_net.weights, zero.policy=TRUE) %>% 
  as.data.frame()

# join local Moran's I results to fishnet
final_net.localMorans <- 
  cbind(local_morans, as.data.frame(final_net)) %>% 
  st_sf() %>%
  dplyr::select(Graffiti_Count = countGraffiti, 
                Local_Morans_I = Ii, 
                P_Value = `Pr(z != E(Ii))`) %>%
  mutate(Significant_Hotspots = ifelse(P_Value <= 0.001, 1, 0)) %>%
  gather(Variable, Value, -geometry)
  
```

### Plotting local Moran's I results

This is a complex code chunk - it's a loop which builds ggplots of local Moran's for each of your `vars`

> What does a significant hot spot tell us about the distribution of burglaries?

```{r moransPlot, echo=TRUE, message=FALSE, warning=FALSE}
## This is just for plotting
vars <- unique(final_net.localMorans$Variable)
varList <- list()

for(i in vars){
  varList[[i]] <- 
    ggplot() +
      geom_sf(data = filter(final_net.localMorans, Variable == i), 
              aes(fill = Value), colour=NA) +
      scale_fill_viridis(name="") +
      labs(title=i) +
      mapTheme(title_size = 14) + theme(legend.position="bottom")}

do.call(grid.arrange,c(varList, ncol = length(vars), top = "Local Morans I statistics, Graffiti"))
```

## Distance to graffiti hot spots

Using NN distance to a hot spot location

```{r predictedHotSpots, echo=TRUE, message=FALSE, warning=FALSE}
# generates warning from NN
final_net <- final_net %>% 
  mutate(graffiti.isSig = 
           ifelse(local_morans[,5] <= 0.001, 1, 0)) %>%
  mutate(graffiti.isSig.dist = 
           nn_function(st_c(st_coid(final_net)),
                       st_c(st_coid(filter(final_net, 
                                           graffiti.isSig == 1))), 
                       k = 1))

ggplot() +
      geom_sf(data = final_net, aes(fill=graffiti.isSig.dist), colour=NA) +
      scale_fill_viridis(name="NN Distance") +
      labs(title="Distance to Graffiti Hotspots") +
      mapTheme()
```

## Correlation Small Multiples
```{r correlationSM, fig.height=15, fig.width= 6, echo=TRUE, message=FALSE, warning=FALSE}
correlation.long <-
  st_drop_geometry(final_net) %>%
    dplyr::select(-uniqueID, -cvID, -name, -District) %>%
    gather(Variable, Value, -countGraffiti)

correlation.cor <-
  correlation.long %>%
    group_by(Variable) %>%
    summarize(correlation = cor(Value, countGraffiti, use = "complete.obs"))
    
ggplot(correlation.long, aes(Value, countGraffiti)) +
  geom_point(size = 0.1) +
  geom_text(data = correlation.cor, aes(label = paste("r =", round(correlation, 2))),
            x=-Inf, y=Inf, vjust = 1.5, hjust = -.1) +
  geom_smooth(method = "lm", se = FALSE, colour = "orange") +
  facet_wrap(~Variable, ncol = 2, scales = "free") +
  labs(title = "Graffiti count as a function of risk factors") +
  plotTheme()
```


## Modeling and CV

Leave One Group Out CV on spatial features

```{r LOGOCVfunc, echo=TRUE, message=FALSE, warning=FALSE}
# Edited function from textbook to get rid of hard-coded variable
crossValidate <- function(dataset, id, dependentVariable, indVariables) {
  
  allPredictions <- data.frame()
  cvID_list <- unique(dataset[[id]])
  
  for (i in cvID_list) {
    
    thisFold <- i
    cat("This hold out fold is", thisFold, "\n")
    
    fold.train <- filter(dataset, dataset[[id]] != thisFold) %>% as.data.frame() %>% 
      dplyr::select(id, geometry, indVariables, dependentVariable)
    fold.test  <- filter(dataset, dataset[[id]] == thisFold) %>% as.data.frame() %>% 
      dplyr::select(id, geometry, indVariables, dependentVariable)
    
    regression <-
      glm(paste0(dependentVariable,"~."), family = "poisson", 
          data = fold.train %>% 
            dplyr::select(-geometry, -id))
    
    thisPrediction <- 
      mutate(fold.test, Prediction = predict(regression, fold.test, type = "response"))
    
    allPredictions <-
      rbind(allPredictions, thisPrediction)
    
  }
  return(st_sf(allPredictions))
}

```


```{r buildModels, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}

## define the variables we want
# reg.vars <- c("Abandoned_Cars.nn", "Illegal_Dumping.nn", 
#               "liquourLicenses.nn", "metroStops.nn", "streetlightsOut.nn", "thefts.nn")
# 
# reg.ss.vars <- c("Abandoned_Cars.nn", "Illegal_Dumping.nn", 
#                 "liquourLicenses.nn", "metroStops.nn", "streetlightsOut.nn", "thefts.nn",
#                 "graffiti.isSig", "graffiti.isSig.dist")
# WITHOUT ABANDONED CARS
reg.vars <- c("Illegal_Dumping.nn", "liquourLicenses.nn", "metroStops.nn", "streetlightsOut.nn", "thefts.nn")

reg.ss.vars <- c("Illegal_Dumping.nn", "liquourLicenses.nn", "metroStops.nn", "streetlightsOut.nn", 
                 "thefts.nn", "graffiti.isSig", "graffiti.isSig.dist")

## RUN REGRESSIONS
reg.cv <- crossValidate(
  dataset = final_net,
  id = "cvID",
  dependentVariable = "countGraffiti",
  indVariables = reg.vars) %>%
    dplyr::select(cvID = cvID, countGraffiti, Prediction, geometry)

reg.ss.cv <- crossValidate(
  dataset = final_net,
  id = "cvID",
  dependentVariable = "countGraffiti",
  indVariables = reg.ss.vars) %>%
    dplyr::select(cvID = cvID, countGraffiti, Prediction, geometry)
  
reg.spatialCV <- crossValidate(
  dataset = final_net,
  id = "name",
  dependentVariable = "countGraffiti",
  indVariables = reg.vars) %>%
    dplyr::select(cvID = name, countGraffiti, Prediction, geometry)

reg.ss.spatialCV <- crossValidate(
  dataset = final_net,
  id = "name",                           
  dependentVariable = "countGraffiti",
  indVariables = reg.ss.vars) %>%
    dplyr::select(cvID = name, countGraffiti, Prediction, geometry)
```

#### Test Accuracy and Generalizability
```{r modelsummaries, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
reg.summary <- 
  rbind(
    mutate(reg.cv,           Error = Prediction - countGraffiti,
                             Regression = "Random k-fold CV: Just Risk Factors"),
                             
    mutate(reg.ss.cv,        Error = Prediction - countGraffiti,
                             Regression = "Random k-fold CV: Spatial Process"),
    
    mutate(reg.spatialCV,    Error = Prediction - countGraffiti,
                             Regression = "Spatial LOGO-CV: Just Risk Factors"),
                             
    mutate(reg.ss.spatialCV, Error = Prediction - countGraffiti,
                             Regression = "Spatial LOGO-CV: Spatial Process")) %>%
    st_sf() 
```


```{r modelSumFigs, echo=TRUE, message=FALSE, warning=FALSE}
# calculate errors by NEIGHBORHOOD
error_by_reg_and_fold <- 
  reg.summary %>%
    group_by(Regression, cvID) %>% 
    summarize(Mean_Error = mean(Prediction - countGraffiti, na.rm = T),
              MAE = mean(abs(Mean_Error), na.rm = T),
              SD_MAE = mean(abs(Mean_Error), na.rm = T)) %>%
  ungroup()

## plot histogram of OOF (out of fold) errors
error_by_reg_and_fold %>%
  ggplot(aes(MAE)) + 
    geom_histogram(bins = 30, colour="black", fill = "#FDE725FF") +
    facet_wrap(~Regression) +  
    geom_vline(xintercept = 0) + scale_x_continuous(breaks = seq(0, 8, by = 1)) + 
    labs(title="Distribution of MAE", subtitle = "k-fold cross validation vs. LOGO-CV",
         x="Mean Absolute Error", y="Count") +
    plotTheme()
```

#### MAE by regression
```{r modelMAEs, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
st_drop_geometry(error_by_reg_and_fold) %>%
  group_by(Regression) %>% 
    summarize(Mean_MAE = round(mean(MAE), 2),
              SD_MAE = round(sd(MAE), 2)) %>%
  kable() %>%
    kable_styling("striped", full_width = F) %>%
    row_spec(2, color = "black", background = "#FDE725FF") %>%
    row_spec(4, color = "black", background = "#FDE725FF")
```
## Racial Context
```{r Census2017, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
census_var2017 <- c("B25026_001E", "B02001_002E", "B19013_001E", "B25058_001E", "B06012_002E")
tractsDC.17 <- get_acs(geography = "tract", 
                           variables = census_var2017, 
                           year=2017, 
                           state=11999,
                           geometry=TRUE, 
                           output="wide") %>%
	dplyr::select(-geometry, -B25026_001M, -B02001_002M, -B19013_001M, -B25058_001M, -B06012_002M) %>% # Remove margin of error columns
	st_transform(crs) %>%
  rename(TotalPop = B25026_001E, 
  			 Whites = B02001_002E,
         MedHHInc = B19013_001E, 
  			 MedRent = B25058_001E,
         TotalPoverty = B06012_002E) %>%
  #dplyr::select(-NAME, -starts_with("B")) %>%
  mutate(pctWhite = ifelse(TotalPop > 0, Whites / TotalPop,0),
         pctPoverty = ifelse(TotalPop > 0, TotalPoverty / TotalPop, 0),
         year = "2017") %>%
	mutate(pctWhite.scaled = pctWhite * 100,
				 pctPoverty.scaled = pctPoverty * 100,
				 raceContext = ifelse(pctWhite > .5, "Majority_White", "Majority_Non_White")) %>%
  dplyr::select(-Whites, -TotalPoverty, -NAME)
```
### Mean Errors by Majority/Minority White Tracts
```{r racialTractErrors, echo=TRUE, message=FALSE, warning=FALSE}
reg.summary %>%
  filter(str_detect(Regression, "LOGO")) %>%
  st_centroid() %>%
  st_join(tractsDC.17) %>%
  na.omit() %>%
  st_drop_geometry() %>%
  group_by(Regression, raceContext) %>%
  summarize(mean.Error = mean(Error, na.rm = T)) %>%
  spread(raceContext, mean.Error) %>%
  kable(caption = "Mean Error by neighborhood racial demographics") %>%
  kable_styling("striped", full_width = F)


```



## Density vs predictions

The `spatstat` function gets us kernel density estimates with varying search radii.

Note that the code here is *different* than in the book - it has been updated to keep up with changes in packages.

```{r kd_dependentVar, echo=TRUE, message=FALSE, warning=FALSE}
# demo of kernel width
graf_ppp <- as.ppp(st_coordinates(graffiti), W = st_bbox(final_net))
graf_KD.1000 <- spatstat.core::density.ppp(graf_ppp, 1000)
graf_KD.1500 <- spatstat.core::density.ppp(graf_ppp, 1500)
graf_KD.2000 <- spatstat.core::density.ppp(graf_ppp, 2000)
graf_KD.df <- rbind(
  mutate(data.frame(rasterToPoints(mask(raster(graf_KD.1000), as(neighborhoods, 'Spatial')))), Legend = "1000 Ft."),
  mutate(data.frame(rasterToPoints(mask(raster(graf_KD.1500), as(neighborhoods, 'Spatial')))), Legend = "1500 Ft."),
  mutate(data.frame(rasterToPoints(mask(raster(graf_KD.2000), as(neighborhoods, 'Spatial')))), Legend = "2000 Ft.")) 

graf_KD.df$Legend <- factor(graf_KD.df$Legend, levels = c("1000 Ft.", "1500 Ft.", "2000 Ft."))

ggplot(data=graf_KD.df, aes(x=x, y=y)) +
  geom_raster(aes(fill=layer)) + 
  facet_wrap(~Legend) +
  coord_sf(crs=st_crs(final_net)) + 
  scale_fill_viridis(name="Density") +
  labs(title = "Kernel density with 3 different search radii") +
  mapTheme(title_size = 14)
```

```{r DV_kd_2017, echo=TRUE, message=FALSE, warning=FALSE}
as.data.frame(graf_KD.1000) %>%
  st_as_sf(coords = c("x", "y"), crs = st_crs(final_net)) %>%
  aggregate(., final_net, mean) %>%
   ggplot() +
     geom_sf(aes(fill=value)) +
     geom_sf(data = sample_n(graffiti, 1500), size = .5) +
     scale_fill_viridis(name = "Density") +
     labs(title = "Kernel density of 2017 graffiti") +
     mapTheme(title_size = 14)
```



## Get 2018 crime data

Let's see how our model performed relative to KD on the following year's data.

```{r crime2018, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
DC311_18 <- st_read("https://opendata.arcgis.com/datasets/2a46f1f1aad04940b83e75e744eb3b09_9.geojson") %>%
  st_transform(crs) %>%
  dplyr::select(-c("INSPECTIONFLAG", "INSPECTIONDATE", "INSPECTORNAME"))

# Pull graffiti from overall 311 data
graffiti18 <- DC311_18 %>%
  filter(grepl("Graffiti", SERVICECODEDESCRIPTION)) %>%
  dplyr::select(Description = SERVICECODEDESCRIPTION)

crime_net_18 <- 
  dplyr::select(graffiti18) %>%
  mutate(countGraffiti = 1) %>%
  aggregate(., fishnet, sum) %>%
  mutate(countGraffiti = replace_na(countGraffiti, 0),
         uniqueID = rownames(.),
         cvID = sample(round(nrow(fishnet) / 24), 
                       size=nrow(fishnet), replace = TRUE))

```

```{r graffiti2018_kd, echo=TRUE, message=FALSE, warning=FALSE}
graf_KDE_sf <- as.data.frame(graf_KD.1000) %>%
  st_as_sf(coords = c("x", "y"), crs = st_crs(final_net)) %>%
  aggregate(., final_net, mean) %>%
  mutate(label = "Kernel Density",
         Risk_Category = ntile(value, 100),
         Risk_Category = case_when(
           Risk_Category >= 90 ~ "90% to 100%",
           Risk_Category >= 70 & Risk_Category <= 89 ~ "70% to 89%",
           Risk_Category >= 50 & Risk_Category <= 69 ~ "50% to 69%",
           Risk_Category >= 30 & Risk_Category <= 49 ~ "30% to 49%",
           Risk_Category >= 1 & Risk_Category  <= 29 ~ "1% to 29%")) %>%
  cbind(
    aggregate(
      dplyr::select(graffiti18) %>% mutate(grafCount = 1), ., sum) %>%
    mutate(grafCount = replace_na(grafCount, 0))) %>%
  dplyr::select(label, Risk_Category, grafCount)
```

Note that this is different from the book, where we pull a model out of a list of models we've created. For your homework, you'll be creating multiple models.

```{r graffiti2017_risk, echo=TRUE, message=FALSE, warning=FALSE}
graf_risk_sf <-
  reg.ss.spatialCV %>%
  mutate(label = "Risk Predictions",
         Risk_Category = ntile(Prediction, 100),
         Risk_Category = case_when(
         Risk_Category >= 90 ~ "90% to 100%",
         Risk_Category >= 70 & Risk_Category <= 89 ~ "70% to 89%",
         Risk_Category >= 50 & Risk_Category <= 69 ~ "50% to 69%",
         Risk_Category >= 30 & Risk_Category <= 49 ~ "30% to 49%",
         Risk_Category >= 1 & Risk_Category <= 29 ~ "1% to 29%")) %>%
  cbind(
    aggregate(
      dplyr::select(graffiti18) %>% mutate(grafCount = 1), ., sum) %>%
      mutate(grafCount = replace_na(grafCount, 0))) %>%
  dplyr::select(label,Risk_Category, grafCount)
```

We don't do quite as well because we don't have very many features, but still pretty good.

```{r RiskPred_v_KD, echo=TRUE, message=FALSE, warning=FALSE}
rbind(graf_KDE_sf, graf_risk_sf) %>%
  na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category, -geometry) %>%
  ggplot() +
    geom_sf(aes(fill = Risk_Category), colour = NA) +
    geom_sf(data = sample_n(graffiti18, 3000), size = .5, colour = "black") +
    facet_wrap(~label, ) +
    scale_fill_viridis(discrete = TRUE) +
    labs(title="Comparison of Kernel Density and Risk Predictions",
         subtitle="2017 graffiti risk predictions; 2018 graffiti") +
    mapTheme(title_size = 14)
```

```{r prediction_vs_kd, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
rbind(graf_KDE_sf, graf_risk_sf) %>%
  st_set_geometry(NULL) %>% na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category) %>%
  group_by(label, Risk_Category) %>%
  summarize(countGraffiti = sum(Value)) %>%
  ungroup() %>%
  group_by(label) %>%
  mutate(Rate_of_test_set_crimes = countGraffiti / sum(countGraffiti)) %>%
    ggplot(aes(Risk_Category,Rate_of_test_set_crimes)) +
      geom_bar(aes(fill=label), position="dodge", stat="identity") +
      scale_fill_viridis(discrete = TRUE) +
      labs(title = "Risk prediction vs. Kernel density, 2018 graffiti") +
      theme(axis.text.x = element_text(angle = 45, vjust = 0.5))
```
