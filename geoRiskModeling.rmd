---
title: "Predictive Policing - Geospatial Risk Modeling"
author: "Johnathan Clementi"
date: "10/29/2021"
output: 
  html_document:
    code_folding: hide
    toc: TRUE
    toc_float: TRUE
---

We are going to run through the code base with just a couple variables in a model - in a slightly simplified workflow

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE)

library(tidyverse)
library(sf)
library(RSocrata)
library(viridis)
library(spatstat)
library(raster)
library(spdep)
library(FNN)
library(grid)
library(gridExtra)
library(knitr)
library(kableExtra)
library(tidycensus)
# functions
root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"
source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

```

## Read in Data from Washington DC

```{r DCBoundaries, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
crs <- 'ESRI:102285' #StatePlane Maryland

# Overall DC Boundary
DCBoundary <- 
  st_read("https://opendata.arcgis.com/datasets/7241f6d500b44288ad983f0942b39663_10.geojson") %>%
  st_transform(crs) 

# DC Neighborhoods from github open data 
## Used in LOOCV 
neighborhoods <- 
  st_read("https://raw.githubusercontent.com/blackmad/neighborhoods/master/washington.geojson") %>%
  st_transform(crs) 

# Police Districts (7 districts) (larger spatial unit)
policeDistricts <- 
  st_read("https://opendata.arcgis.com/datasets/d2a63e5246ff41bdaca8ea9be95c8a4b_9.geojson") %>%
  st_transform(crs) %>%
  dplyr::select(District = DISTRICT) %>%
  mutate(District = as.character(District))
  
# Each police district has about 3 Sectors (smaller spatial unit) 
policeSectors <- 
  st_read("https://opendata.arcgis.com/datasets/6ac17c2ff8cc4e20b3768dd1b98adf7a_23.geojson") %>%
  st_transform(crs) %>%
  dplyr::select(District = NAME)

# Format police sectors in usable way
policeSectors$District <- paste0("0", str_replace(policeSectors$District, "D", "0"))


bothPoliceUnits <- rbind(mutate(policeDistricts, Legend = "Police Districts"), 
                         mutate(policeSectors, Legend = "Police Sectors"))

```

```{r graffiti, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
# DC 311 Complaints
DC311 <- 
  st_read("https://opendata.arcgis.com/datasets/19905e2b0e1140ec9ce8437776feb595_8.geojson") %>%
  st_transform(crs)  %>%
  dplyr::select(-c("INSPECTIONFLAG", "INSPECTIONDATE", "INSPECTORNAME")) 

DC311Vars <- data.frame(table(DC311$SERVICECODEDESCRIPTION))

# Pull graffiti from overall 311 data
graffiti <- DC311 %>%
  filter(grepl("Graffiti", SERVICECODEDESCRIPTION)) %>%
  dplyr::select(Description = SERVICECODEDESCRIPTION,
                Count = SERVICECALLCOUNT)

```

## visualizing point data

Plotting point data and density

> How do we analyze point data?
>
> Are there other geometries useful to represent point locations?
  - aggregate by 

```{r pointDens, echo=TRUE, fig.height=6, fig.width=7, message=FALSE, warning=FALSE}

grid.arrange(ncol=2,
ggplot() + 
  geom_sf(data = DCBoundary) +
  geom_sf(data = graffiti, colour="red", size=0.1, show.legend = "point") +
  labs(title= "Graffiti Complaints,\nWashington DC - 2017") +
  mapTheme(title_size = 14),

ggplot() + 
  geom_sf(data = DCBoundary, fill = "grey40") +
  stat_density2d(data = data.frame(st_coordinates(graffiti)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_viridis() +
  scale_alpha(range = c(0.00, 0.35), guide = FALSE) +
  labs(title = "Density of Graffiti Complaints\nWashington DC - 2017") +
  mapTheme(title_size = 14) + theme(legend.position = "none"))

```

## Creating a fishnet grid

```{r fishnet echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
## using {sf} to create the grid
## Note the `.[DCBoundary] %>% ` line. This is needed to clip the grid to our data
fishnet <- 
  st_make_grid(DCBoundary,
               cellsize = 500, 
               square = TRUE) %>%
  .[DCBoundary] %>%            # <- MDH Added
  st_sf() %>%
  mutate(uniqueID = rownames(.))
```

### Aggregate points to the fishnet

```{r graffitiAggregate, echo=TRUE echo=TRUE, message=FALSE, warning=FALSE}
## add a value of 1 to each crime, sum them with aggregate

crime_net <- 
  dplyr::select(graffiti) %>% 
  mutate(countGraffiti = 1) %>% 
  aggregate(., fishnet, sum) %>%
  mutate(countGraffiti = replace_na(countGraffiti, 0),
         uniqueID = rownames(.),
         cvID = sample(round(nrow(fishnet) / 24), 
                       size=nrow(fishnet), replace = TRUE))

ggplot() +
  geom_sf(data = crime_net, aes(fill = countGraffiti), color = NA) +
  scale_fill_viridis() +
  labs(title = "Count of Graffiti complaints for the fishnet") +
  mapTheme()

```

## Modeling Spatial Features

> What features would be helpful in predicting the location of burglaries?
>
> What might these features be problematic?
>
> hint: for all the reasons we learned in class


```{r pullGraffitiPredictors, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}

# Abandoned cars (includes both public & private land)
abandonedCars <- DC311 %>%
  filter(grepl("Abandoned Vehicle", SERVICECODEDESCRIPTION)) %>%
  mutate(Legend = "Abandoned_Car", Count = 1) %>%
  dplyr::select(Legend, Count)

# Pull Illegal Dumping from overall 311 data
Illegal_Dumping <- DC311 %>%
  filter(grepl("Illegal Dumping", SERVICECODEDESCRIPTION)) %>%
  mutate(Legend = "Illegal_Dumping", Count = 1) %>%
  dplyr::select(Legend, Count)

# Maybe drunk people are more likely to tag?
liquourLicenses <- st_read("https://opendata.arcgis.com/datasets/cabe9dcef0b344518c7fae1a3def7de1_5.geojson") %>%
  st_transform(crs) %>%
  mutate(Legend = "liquourLicenses", Count = 1) %>%
  dplyr::select(Legend, Count)

# Metro stations 
metroStops <- st_read("https://opendata.arcgis.com/datasets/54018b7f06b943f2af278bbe415df1de_52.geojson") %>%
	st_transform(crs) %>%
  mutate(Legend = "metroStops", Count = 1) %>%
  dplyr::select(Legend, Count)

# 311 complaints regarding streetlight outages
streetlightsOut <- DC311%>%
  filter(grepl("Streetlight", SERVICECODEDESCRIPTION)) %>%
  mutate(Legend = "streetlightsOut", Count = 1) %>%
  dplyr::select(Legend, Count)

# Major crimes 
thefts <- st_read("https://opendata.arcgis.com/datasets/6af5cb8dc38e4bcbac8168b27ee104aa_38.geojson") %>%
  st_transform(crs) %>%
	filter(grepl('THEFT',OFFENSE)) %>%
	mutate(Legend = "thefts", Count = 1) %>%
	dplyr::select(Legend, Count) 

```

#### How we aggregate a feature to our fishnet

This is an important chunk of code with some unfamiliar lines. We'll step through it.

```{r echo=TRUE, message=FALSE, warning=FALSE, results='hide'}

vars_net <- rbind(abandonedCars, Illegal_Dumping, liquourLicenses, metroStops, streetlightsOut, thefts) %>%
  st_join(., fishnet, join=st_within) %>%
  st_drop_geometry() %>%
  group_by(uniqueID, Legend) %>%
  summarize(count = n()) %>%
  full_join(fishnet, by = "uniqueID") %>%
  spread(Legend, count, fill=0) %>%
  st_sf() %>%
  dplyr::select(-`<NA>`) %>%
  na.omit() %>%
  ungroup()

# vars_net <- allPredictors %>%
#   spatially join allPredictors points to the fishnet polygon they are within %>%
#   drop the geometry attribute %>%
#   group_by each cells ID and the name of the feature %>%
#   summarize count the number of each point per grid cell %>%
#   join that summary back to spatial fishnet by cell ID %>%
#   "spread" from long to wide format and make column of our point count %>%
#   tell R that this should be an sf object %>%
#   remove a fussy column that appears b/c of NA %>%
#   get rid of rows with an NA in any column %>%
#   remove grouping so you are not tripped up later
```

## Nearest Neighbor Feature

> Review: what is NN and what does `k` represent in this function?

```{r}
# convinience to reduce length of function names.
st_c    <- st_coordinates
st_coid <- st_centroid

# Get coordinates of fishnet centroids
netCentroidCoords <- st_c(st_coid(vars_net))

# Get coordinates for k-nearest neighbor functions below
accoords <- st_c(abandonedCars)
idcoords <- st_c(Illegal_Dumping)
lqcoords <- st_c(liquourLicenses)
mscoords <- st_c(metroStops)
slcoords <- st_c(streetlightsOut)
tcoords <- st_c(thefts)

# Define number of nearest neighbors to look for
k = 3
                    

## create NN from abandoned cars
vars_net <- vars_net %>%
    mutate(Abandoned_Cars.nn = nn_function(netCentroidCoords, accoords, k), 
           Illegal_Dumping.nn = nn_function(netCentroidCoords, idcoords, k),
           liquourLicenses.nn = nn_function(netCentroidCoords, lqcoords, k),
           metroStops.nn = nn_function(netCentroidCoords, mscoords, k),
           streetlightsOut.nn = nn_function(netCentroidCoords, slcoords, k),
           thefts.nn = nn_function(netCentroidCoords, tcoords, k)
          )                                             

```


```{r}
## Visualize the NN feature
vars_net.long.nn <- 
  dplyr::select(vars_net, ends_with(".nn")) %>%
    gather(Variable, value, -geometry)

vars <- unique(vars_net.long.nn$Variable)
mapList <- list()

for(i in vars){
  mapList[[i]] <- 
    ggplot() +
      geom_sf(data = filter(vars_net.long.nn, Variable == i), aes(fill=value), colour=NA) +
      scale_fill_viridis(name="") +
      labs(title=i) +
      mapTheme()}

do.call(grid.arrange,c(mapList, ncol = 3, top = "Average Distance to nearest neighbor(s) risk factors by fishnet"))
```

## Join NN feature to our fishnet

Since the counts were aggregated to each cell by `uniqueID` we can use that to join the counts to the fishnet.

```{r}
## important to drop the geometry from joining features
final_net <-
  left_join(crime_net, st_drop_geometry(vars_net), by="uniqueID") 

```

### Join in areal data

Using spatial joins to join *centroids* of fishnets to polygon for neighborhoods and districts.

> What issues arise when we try to join polygons to polygons in space? End up with weird slices of polygons

```{r}

final_net <-
  st_centroid(final_net) %>%
    st_join(dplyr::select(neighborhoods, name), by = "uniqueID") %>%
    st_join(dplyr::select(policeDistricts, District), by = "uniqueID") %>%
      st_drop_geometry() %>%
      left_join(dplyr::select(final_net, geometry, uniqueID)) %>%
      st_sf() %>%
  na.omit()

```



## Local Moran's I for fishnet grid cells

using {spdep} package to to build neighborhood weights and list to calculate local Moran's I.

Note that the code here is *different* than in the book - it has been updated to keep up with changes in packages.

> What is the difference between local and global Moran's I?

A little in depth version of the chunk below can be found:

Mendez C. (2020). Spatial autocorrelation analysis in R. R Studio/RPubs. Available at <https://rpubs.com/quarcs-lab/spatial-autocorrelation>

```{r}
## generates warnings from PROJ issues
## {spdep} to make polygon to neighborhoods... 
final_net.nb <- poly2nb(as_Spatial(final_net), queen=TRUE)
## ... and neighborhoods to list of weigths
final_net.weights <- nb2listw(final_net.nb, style="W", zero.policy=TRUE)

# print(final_net.weights, zero.policy=TRUE)
```

```{r}
## see ?localmoran
local_morans <- localmoran(final_net$countGraffiti, final_net.weights, zero.policy=TRUE) %>% 
  as.data.frame()

# join local Moran's I results to fishnet
final_net.localMorans <- 
  cbind(local_morans, as.data.frame(final_net)) %>% 
  st_sf() %>%
  dplyr::select(Graffiti_Count = countGraffiti, 
                Local_Morans_I = Ii, 
                P_Value = `Pr(z != E(Ii))`) %>%
  mutate(Significant_Hotspots = ifelse(P_Value <= 0.001, 1, 0)) %>%
  gather(Variable, Value, -geometry)
  
```

### Plotting local Moran's I results

This is a complex code chunk - it's a loop which builds ggplots of local Moran's for each of your `vars`

> What does a significant hot spot tell us about the distribution of burglaries?

```{r}
## This is just for plotting
vars <- unique(final_net.localMorans$Variable)
varList <- list()

for(i in vars){
  varList[[i]] <- 
    ggplot() +
      geom_sf(data = filter(final_net.localMorans, Variable == i), 
              aes(fill = Value), colour=NA) +
      scale_fill_viridis(name="") +
      labs(title=i) +
      mapTheme(title_size = 14) + theme(legend.position="bottom")}

do.call(grid.arrange,c(varList, ncol = length(vars), top = "Local Morans I statistics, Graffiti"))
```

## Distance to graffiti hot spots

Using NN distance to a hot spot location

```{r}
# generates warning from NN
final_net <- final_net %>% 
  mutate(graffiti.isSig = 
           ifelse(local_morans[,5] <= 0.001, 1, 0)) %>%
  mutate(graffiti.isSig.dist = 
           nn_function(st_c(st_coid(final_net)),
                       st_c(st_coid(filter(final_net, 
                                           graffiti.isSig == 1))), 
                       k = 1))

ggplot() +
      geom_sf(data = final_net, aes(fill=graffiti.isSig.dist), colour=NA) +
      scale_fill_viridis(name="NN Distance") +
      labs(title="Distance to Graffiti Hotspots") +
      mapTheme()
```

## Correlation Small Multiples
```{r correlationSM, fig.height=10, fig.width= 6}
correlation.long <-
  st_drop_geometry(final_net) %>%
    dplyr::select(-uniqueID, -cvID, -name, -District) %>%
    gather(Variable, Value, -countGraffiti)

correlation.cor <-
  correlation.long %>%
    group_by(Variable) %>%
    summarize(correlation = cor(Value, countGraffiti, use = "complete.obs"))
    
ggplot(correlation.long, aes(Value, countGraffiti)) +
  geom_point(size = 0.1) +
  geom_text(data = correlation.cor, aes(label = paste("r =", round(correlation, 2))),
            x=-Inf, y=Inf, vjust = 1.5, hjust = -.1) +
  geom_smooth(method = "lm", se = FALSE, colour = "black") +
  facet_wrap(~Variable, ncol = 2, scales = "free") +
  labs(title = "Graffiti count as a function of risk factors") +
  plotTheme()
```


## Modeling and CV

Leave One Group Out CV on spatial features

```{r sults='hide'}

## define the variables we want
reg.vars <- c("Abandoned_Cars.nn", "Illegal_Dumping.nn", 
              "liquourLicenses.nn", "MetroStops", "streetlightsOut.nn", "thefts.nn")

reg.ss.vars <- c("Abandoned_Cars.nn", "Illegal_Dumping.nn", 
                "liquourLicenses.nn", "MetroStops", "streetlightsOut.nn", "thefts.nn",
                "graffiti.isSig", "graffiti.isSig.dist")

## RUN REGRESSIONS
reg.ss.spatialCV <- crossValidate(
  dataset = final_net,
  id = "name",                           
  dependentVariable = "countGraffiti",
  indVariables = reg.ss.vars) %>%
    dplyr::select(cvID = name, countGraffiti, Prediction, geometry)
```

```{r}
# calculate errors by NEIGHBORHOOD
error_by_reg_and_fold <- 
  reg.ss.spatialCV %>%
    group_by(cvID) %>% 
    summarize(Mean_Error = mean(Prediction - countGraffiti, na.rm = T),
              MAE = mean(abs(Mean_Error), na.rm = T),
              SD_MAE = mean(abs(Mean_Error), na.rm = T)) %>%
  ungroup()

error_by_reg_and_fold %>% 
  arrange(desc(MAE))
error_by_reg_and_fold %>% 
  arrange(MAE)

## plot histogram of OOF (out of fold) errors
error_by_reg_and_fold %>%
  ggplot(aes(MAE)) + 
    geom_histogram(bins = 30, colour="black", fill = "#FDE725FF") +
  scale_x_continuous(breaks = seq(0, 11, by = 1)) + 
    labs(title="Distribution of MAE", subtitle = "LOGO-CV",
         x="Mean Absolute Error", y="Count") 
```

## Density vs predictions

The `spatstat` function gets us kernal density estimates with varying search radii.

Note that the code here is *different* than in the book - it has been updated to keep up with changes in packages.

```{r}
# demo of kernel width
burg_ppp <- as.ppp(st_coordinates(burglaries), W = st_bbox(final_net))
burg_KD.1000 <- spatstat.core::density.ppp(burg_ppp, 1000)
burg_KD.1500 <- spatstat.core::density.ppp(burg_ppp, 1500)
burg_KD.2000 <- spatstat.core::density.ppp(burg_ppp, 2000)
burg_KD.df <- rbind(
  mutate(data.frame(rasterToPoints(mask(raster(burg_KD.1000), as(neighborhoods, 'Spatial')))), Legend = "1000 Ft."),
  mutate(data.frame(rasterToPoints(mask(raster(burg_KD.1500), as(neighborhoods, 'Spatial')))), Legend = "1500 Ft."),
  mutate(data.frame(rasterToPoints(mask(raster(burg_KD.2000), as(neighborhoods, 'Spatial')))), Legend = "2000 Ft.")) 

burg_KD.df$Legend <- factor(burg_KD.df$Legend, levels = c("1000 Ft.", "1500 Ft.", "2000 Ft."))

ggplot(data=burg_KD.df, aes(x=x, y=y)) +
  geom_raster(aes(fill=layer)) + 
  facet_wrap(~Legend) +
  coord_sf(crs=st_crs(final_net)) + 
  scale_fill_viridis(name="Density") +
  labs(title = "Kernel density with 3 different search radii") +
  mapTheme(title_size = 14)
```

```{r}

as.data.frame(burg_KD.1000) %>%
  st_as_sf(coords = c("x", "y"), crs = st_crs(final_net)) %>%
  aggregate(., final_net, mean) %>%
   ggplot() +
     geom_sf(aes(fill=value)) +
     geom_sf(data = sample_n(burglaries, 1500), size = .5) +
     scale_fill_viridis(name = "Density") +
     labs(title = "Kernel density of 2017 burglaries") +
     mapTheme(title_size = 14)
```




## Get 2018 crime data

Let's see how our model performed relative to KD on the following year's data.

```{r}
burglaries18 <- 
  read.socrata("https://data.cityofchicago.org/Public-Safety/Crimes-2018/3i3m-jwuy") %>% 
  filter(Primary.Type == "BURGLARY" & 
         Description == "FORCIBLE ENTRY") %>%
  mutate(x = gsub("[()]", "", Location)) %>%
  separate(x,into= c("Y","X"), sep=",") %>%
  mutate(X = as.numeric(X),
         Y = as.numeric(Y)) %>% 
  na.omit %>%
  st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
  st_transform(crs) %>% 
  distinct() %>%
  .[fishnet,]
```

```{r}
burg_KDE_sf <- as.data.frame(burg_KD.1000) %>%
  st_as_sf(coords = c("x", "y"), crs = st_crs(final_net)) %>%
  aggregate(., final_net, mean) %>%
  mutate(label = "Kernel Density",
         Risk_Category = ntile(value, 100),
         Risk_Category = case_when(
           Risk_Category >= 90 ~ "90% to 100%",
           Risk_Category >= 70 & Risk_Category <= 89 ~ "70% to 89%",
           Risk_Category >= 50 & Risk_Category <= 69 ~ "50% to 69%",
           Risk_Category >= 30 & Risk_Category <= 49 ~ "30% to 49%",
           Risk_Category >= 1 & Risk_Category  <= 29 ~ "1% to 29%")) %>%
  cbind(
    aggregate(
      dplyr::select(burglaries18) %>% mutate(burgCount = 1), ., sum) %>%
    mutate(burgCount = replace_na(burgCount, 0))) %>%
  dplyr::select(label, Risk_Category, burgCount)
```

Note that this is different from the book, where we pull a model out of a list of models we've created. For your homework, you'll be creating multiple models.

```{r}
burg_risk_sf <-
  reg.ss.spatialCV %>%
  mutate(label = "Risk Predictions",
         Risk_Category = ntile(Prediction, 100),
         Risk_Category = case_when(
         Risk_Category >= 90 ~ "90% to 100%",
         Risk_Category >= 70 & Risk_Category <= 89 ~ "70% to 89%",
         Risk_Category >= 50 & Risk_Category <= 69 ~ "50% to 69%",
         Risk_Category >= 30 & Risk_Category <= 49 ~ "30% to 49%",
         Risk_Category >= 1 & Risk_Category <= 29 ~ "1% to 29%")) %>%
  cbind(
    aggregate(
      dplyr::select(burglaries18) %>% mutate(burgCount = 1), ., sum) %>%
      mutate(burgCount = replace_na(burgCount, 0))) %>%
  dplyr::select(label,Risk_Category, burgCount)
```

We don't do quite as well because we don't have very many features, but still pretty good.

```{r}
rbind(burg_KDE_sf, burg_risk_sf) %>%
  na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category, -geometry) %>%
  ggplot() +
    geom_sf(aes(fill = Risk_Category), colour = NA) +
    geom_sf(data = sample_n(burglaries18, 3000), size = .5, colour = "black") +
    facet_wrap(~label, ) +
    scale_fill_viridis(discrete = TRUE) +
    labs(title="Comparison of Kernel Density and Risk Predictions",
         subtitle="2017 burglar risk predictions; 2018 burglaries") +
    mapTheme(title_size = 14)
```

```{r}
rbind(burg_KDE_sf, burg_risk_sf) %>%
  st_set_geometry(NULL) %>% na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category) %>%
  group_by(label, Risk_Category) %>%
  summarize(countGraffiti = sum(Value)) %>%
  ungroup() %>%
  group_by(label) %>%
  mutate(Rate_of_test_set_crimes = countGraffiti / sum(countGraffiti)) %>%
    ggplot(aes(Risk_Category,Rate_of_test_set_crimes)) +
      geom_bar(aes(fill=label), position="dodge", stat="identity") +
      scale_fill_viridis(discrete = TRUE) +
      labs(title = "Risk prediction vs. Kernel density, 2018 burglaries") +
      theme(axis.text.x = element_text(angle = 45, vjust = 0.5))
```
